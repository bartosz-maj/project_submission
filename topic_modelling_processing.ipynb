{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e6bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import scipy.sparse\n",
    "from datasets import load_dataset, Dataset\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80ada75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec22283/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec22283/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec22283/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /home/ec22283/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec22283/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading important NLTK packagees. \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c57b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the comments dataset \n",
    "comments = pd.read_csv(\"comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc65b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "def num(word):\n",
    "    for letter in word:\n",
    "        if letter.isdigit():\n",
    "            return True \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c89e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process_text function. \n",
    "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
    "lem = lemmatizer = WordNetLemmatizer()\n",
    "def process_text(document):\n",
    "    \n",
    "    # Convert all words to lowercase. \n",
    "    document = document.lower()\n",
    "    # Remove all punctuation \n",
    "    no_punc = tokenizer.tokenize(document)\n",
    "    #Remove all digits. \n",
    "    no_num = [word for word in no_punc if num(word) == False]\n",
    "    # Remove words which are not longer than a single world.\n",
    "    single = [word for word in no_num if len(word) > 1]\n",
    "    # lemmatize all words. \n",
    "    lemmed = [lem.lemmatize(word) for word in single]\n",
    "    return lemmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf4c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert comment series into comment list.  \n",
    "working_text = comments[\"body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e030f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4600698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply process_text function. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m [process_text(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(working_text)]\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Apply process_text function. \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(working_text)]\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mprocess_text\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     13\u001b[0m single \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m no_num \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# lemmatize all words. \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m lemmed \u001b[38;5;241m=\u001b[39m [lem\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m single]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lemmed\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m single \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m no_num \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# lemmatize all words. \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m lemmed \u001b[38;5;241m=\u001b[39m [\u001b[43mlem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m single]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lemmed\n",
      "File \u001b[0;32m~/myenv01/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/myenv01/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:2100\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m filter_forms([form] \u001b[38;5;241m+\u001b[39m exceptions[form])\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m forms \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mform\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;66;03m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m results \u001b[38;5;241m=\u001b[39m filter_forms([form] \u001b[38;5;241m+\u001b[39m forms)\n",
      "File \u001b[0;32m~/myenv01/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:2076\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[0;32m-> 2076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   2077\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[1;32m   2078\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[1;32m   2079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[1;32m   2080\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m form\u001b[38;5;241m.\u001b[39mendswith(old)\n\u001b[1;32m   2081\u001b[0m     ]\n",
      "File \u001b[0;32m~/myenv01/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:2080\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[1;32m   2076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   2077\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[1;32m   2078\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[1;32m   2079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[0;32m-> 2080\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2081\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply process_text function. \n",
    "processed_text = [process_text(doc) for doc in tqdm(working_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd925cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each sentence. \n",
    "processed_join = [\" \".join(doc) for doc in tqdm(processed_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df699f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurence of each word across the corpus.\n",
    "counter = Counter()\n",
    "for doc in tqdm(processed_text):\n",
    "    # Each comment is turned into a set, removing duplicates. This ensures the counter is returning \n",
    "    # document frequency of each word rather than total corpus frequency. \n",
    "    counter.update(set(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the counter. \n",
    "ordered_counter = counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lists of words which either occur in over 40% of comments or less than 100 comments. \n",
    "under_list = [word for (word, count) in ordered_counter if count < 100]\n",
    "over_list = [word for (word, count) in ordered_counter if count > (len(processed_text)*0.4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gensim processing functions. \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating custom stopword list. \n",
    "custom_stop = [\"www\", \"https\", \"com\", \"http\", \"people\", \"like\", \"gt\", \"___ \", \"amp\", \"org\", \"ve\", \"en\", \"httml\", \"np\", \"pdf\"]\n",
    "# Instancing default Gensim stopword list. \n",
    "all_stopwords_gensim = gensim.parsing.preprocessing.STOPWORDS\n",
    "# Combining list of all words to be removed into one stopword list.\n",
    "all_stopwords_gensim = STOPWORDS.union(set(under_list + over_list + custom_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad41ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopword removal to process. \n",
    "process = [remove_stopwords(doc, stopwords = all_stopwords_gensim) for doc in tqdm(processed_join)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9783a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each comment into words. \n",
    "split_process = [doc.split() for doc in tqdm(process)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits into batches. \n",
    "noun_one = split_process[0:1000000]\n",
    "noun_two = split_process[1000000:2000000]\n",
    "noun_three = split_process[2000000:3000000]\n",
    "noun_four = split_process[3000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows length of all batches combined. \n",
    "len(noun_one) + len(noun_two) + len(noun_three) + len(noun_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert statements to ensure respective eelements of the batches and the original list are equal. \n",
    "assert(noun_one[-1] == split_process[999999])\n",
    "assert(noun_two[-1] == split_process[1999999])\n",
    "assert(noun_three[-1] == split_process[2999999])\n",
    "assert(noun_four[-1] == split_process[-1])\n",
    "assert((len(noun_one) + len(noun_two) + len(noun_three) + len(noun_four)) == len(split_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noun removal function to remove all non-nouns. \n",
    "def nouns_only(text):\n",
    "    return [word for (word, tag) in TextBlob(text).tags if tag == \"NN\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply noun removal function and upload batch to huggingface. \n",
    "#nouns_one_processed = [nouns_only(\" \".join(doc)) for doc in tqdm(noun_one)]\n",
    "#processed_dataset = Dataset.from_pandas(pd.DataFrame({\"nouns\":nouns_one_processed}))\n",
    "#processed_dataset.push_to_hub(\"bartoszmaj/nouns_one\")\n",
    "\n",
    "#import_data = load_dataset(\"bartoszmaj/nouns_one\")\n",
    "#import_df = pd.DataFrame({\"body\":import_data[\"train\"][\"nouns\"]})\n",
    "#nouns_one = import_df[\"body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns_two_processed = [nouns_only(\" \".join(doc)) for doc in tqdm(noun_two)]\n",
    "#processed_dataset = Dataset.from_pandas(pd.DataFrame({\"nouns\":nouns_two_processed}))\n",
    "#processed_dataset.push_to_hub(\"bartoszmaj/nouns_two\")\n",
    "\n",
    "#import_data = load_dataset(\"bartoszmaj/nouns_two\")\n",
    "#import_df = pd.DataFrame({\"body\":import_data[\"train\"][\"nouns\"]})\n",
    "#nouns_two = import_df[\"body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns_three_processed = [nouns_only(\" \".join(doc)) for doc in tqdm(noun_three)]\n",
    "#processed_dataset = Dataset.from_pandas(pd.DataFrame({\"nouns\":nouns_three_processed}))\n",
    "#processed_dataset.push_to_hub(\"bartoszmaj/nouns_three\")\n",
    "\n",
    "#import_data = load_dataset(\"bartoszmaj/nouns_three\")\n",
    "#import_df = pd.DataFrame({\"body\":import_data[\"train\"][\"nouns\"]})\n",
    "#nouns_three = import_df[\"body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a694b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns_four_processed = [nouns_only(\" \".join(doc)) for doc in tqdm(noun_four)]\n",
    "#processed_dataset = Dataset.from_pandas(pd.DataFrame({\"nouns\":nouns_four_processed}))\n",
    "#processed_dataset.push_to_hub(\"bartoszmaj/nouns_four\")\n",
    "\n",
    "#import_data = load_dataset(\"bartoszmaj/nouns_four\")\n",
    "#import_df = pd.DataFrame({\"body\":import_data[\"train\"][\"nouns\"]})\n",
    "#nouns_four = import_df[\"body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12248299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all processed nouns \n",
    "nouns_imported = nouns_one + nouns_two + nouns_three + nouns_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cc344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload processed comments to huggingface. \n",
    "#nouns_full = Dataset.from_pandas(pd.DataFrame({\"nouns\":nouns_imported}))\n",
    "#nouns_full.push_to_hub(\"bartoszmaj/nouns_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512d5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ddc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
